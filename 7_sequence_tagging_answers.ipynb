{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sequence Tagging\n",
    "\n",
    "In this lab we will train a part-of-speech (POS) tagger using an HMM and then an RNN.\n",
    "\n",
    "### Outcomes\n",
    "* Be able to train and apply an HMM.\n",
    "* Understand what the steps of Viterbi are doing.\n",
    "* Recognise how to adapt Pytorch models to use RNN layers and perform sequence tagging with neural networks.\n",
    "\n",
    "### Overview\n",
    "\n",
    "The first part of the notebook loads a POS dataset from the NLTK library.\n",
    "The second part implements and tests an HMM POS tagger.\n",
    "The third part adapts the neural network code from last week to train the RNN as a POS tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing the PoS Tagging Data\n",
    "\n",
    "To train our POS tagger, we will use the Brown corpus, which contains many different sources of English text (books, essays, newspaper articles, government documents...) collected and hand-labelled by linguists in 1967."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/es1595/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/es1595/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')  # download Brown corpus\n",
    "nltk.download('universal_tagset')  # download the universal tagset: the 17 PoS tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the dataset into train and test, then re-format it so that each split is represented by a list of sentences and a list of tag sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 45872\n",
      "Number of test sentences: 11468\n",
      "Number of tagged words in the training set: 45872\n",
      "Number of tagged words in the test set: 11468\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "nltk_data = list(brown.tagged_sents(tagset='universal'))\n",
    "train_set, test_set = train_test_split(\n",
    "    nltk_data,\n",
    "    train_size=0.80,\n",
    "    test_size=0.20,\n",
    "    random_state=101\n",
    ")\n",
    "print(f'Number of training sentences: {len(train_set)}')\n",
    "print(f'Number of test sentences: {len(test_set)}')\n",
    "\n",
    "# Separate the labels from the text\n",
    "train_toks = []\n",
    "train_tags = []\n",
    "for tagged_sentence in train_set:\n",
    "    sentence_toks = []\n",
    "    sentence_tags = []\n",
    "    for token, tag in tagged_sentence:\n",
    "        sentence_toks.append(token)\n",
    "        sentence_tags.append(tag)\n",
    "\n",
    "    train_toks.append(sentence_toks)\n",
    "    train_tags.append(sentence_tags)\n",
    "\n",
    "test_toks = []\n",
    "test_tags = []\n",
    "for tagged_sentence in test_set:\n",
    "    sentence_toks = []\n",
    "    sentence_tags = []\n",
    "    for token, tag in tagged_sentence:\n",
    "        sentence_toks.append(token)\n",
    "        sentence_tags.append(tag)\n",
    "    test_toks.append(sentence_toks)\n",
    "    test_tags.append(sentence_tags)\n",
    "\n",
    "print(f'Number of tagged words in the training set: {len(train_toks)}')\n",
    "print(f'Number of tagged words in the test set: {len(test_toks)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use LabelEncoder to map the tokens to IDs and convert the sentences to sequences of token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 45872/45872 [00:00<00:00, 1022931.36it/s]\n",
      "100%|███████████████████████████████████████████████████| 11468/11468 [00:00<00:00, 1146582.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary is 56057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Convert the tokens to IDs in a vocabulary ready for input to our models\n",
    "tok_encoder = LabelEncoder()\n",
    "\n",
    "all_train_words = [word for sentence in train_toks for word in sentence]\n",
    "all_test_words = [word for sentence in test_toks for word in sentence]\n",
    "all_words = all_train_words + all_test_words\n",
    "tok_encoder.fit(all_words)\n",
    "\n",
    "all_encoded = tok_encoder.transform(all_train_words)\n",
    "\n",
    "train_toks_encoded = []\n",
    "start_idx = 0\n",
    "for sentence in tqdm(train_toks):\n",
    "    train_toks_encoded.append(all_encoded[start_idx:start_idx+len(sentence)])\n",
    "    start_idx += len(sentence)\n",
    "\n",
    "all_encoded = tok_encoder.transform(all_test_words)\n",
    "\n",
    "test_toks_encoded = []\n",
    "start_idx = 0\n",
    "for sentence in tqdm(test_toks):\n",
    "    test_toks_encoded.append(all_encoded[start_idx:start_idx+len(sentence)])\n",
    "    start_idx += len(sentence)\n",
    "\n",
    "V = len(tok_encoder.classes_)\n",
    "print(f'Size of vocabulary is {V}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final preprocessing step is to map the tags (class labels) to numerical IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the tags from their names to numbers\n",
    "tag_encoder = LabelEncoder()\n",
    "tag_encoder.fit([tag for sentence in train_tags for tag in sentence])\n",
    "train_tags_encoded = [tag_encoder.transform(sentence) for sentence in train_tags]\n",
    "test_tags_encoded = [tag_encoder.transform(sentence) for sentence in test_tags]\n",
    "\n",
    "num_tags = len(tag_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Implementing the HMM\n",
    "\n",
    "Now, we are going to put together an HMM by estimating the different variables in the model from the training set. \n",
    "\n",
    "**TO-DO 2.1:** Count the state transitions and starting state occurrences in the training set and store the counts in the `transitions` and `start_states` matrices below. In `transitions`, rows correspond to states at time t-1, the columns to the following state at time t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 45872/45872 [00:00<00:00, 80211.48it/s]\n"
     ]
    }
   ],
   "source": [
    "transitions = np.zeros((num_tags, num_tags))\n",
    "start_states = np.zeros(num_tags)\n",
    "\n",
    "for sentence_tags in tqdm(train_tags_encoded):\n",
    "    for i, tag in enumerate(sentence_tags):\n",
    "        if i==0:\n",
    "            start_states[tag] += 1\n",
    "            continue\n",
    "        ### WRITE YOUR OWN CODE HERE\n",
    "        transitions[tag, sentence_tags[i-1]] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 2.2:** Normalise the transition and start state counts to estimate the conditional probabilities in the transition matrix and \\pi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "transitions /= np.sum(transitions, 1)[:, None]\n",
    "start_states /= np.sum(start_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 2.3:** Count the number of occurrences of each word type given each tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45872it [00:00, 74530.45it/s]\n"
     ]
    }
   ],
   "source": [
    "observations = np.zeros((num_tags, V))\n",
    "\n",
    "for i, sentence_toks in tqdm(enumerate(train_toks_encoded)):\n",
    "    sentence_tags = train_tags_encoded[i]\n",
    "    for j, tok in enumerate(sentence_toks):\n",
    "        tag = sentence_tags[j]\n",
    "        # WRITE YOUR OWN CODE HERE\n",
    "        observations[tag, tok] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 2.4:** Normalise the observation counts to obtain the observation probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#WRITE YOUR OWN CODE HERE\n",
    "observations /= np.sum(observations, 1)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 2.5:** Check the implementation of viterbi below for errors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def viterbi(observed_seq, num_tags, start_probs, transition_probs, observation_probs):\n",
    "    eps = 1e-7\n",
    "\n",
    "    num_obs = observed_seq.shape[0]\n",
    "\n",
    "    # Initialise the V and backpointers\n",
    "    V = np.zeros((num_obs, num_tags))\n",
    "    backpointer = np.zeros((num_obs, num_tags))\n",
    "\n",
    "    # For the first data point in the sequence:\n",
    "    V[0, :] = start_probs * observation_probs[:, observed_seq[0]]\n",
    "\n",
    "    # Run Viterbi forward for t > 0\n",
    "    for t in range(1, num_obs):\n",
    "\n",
    "        for state in range(num_tags):\n",
    "            # probabilities for all the sequences leading to this state at time t\n",
    "            seq_prob = V[t-1, :] * transition_probs[:, state]\n",
    "\n",
    "            # Choose the most likely sequence\n",
    "            max_seq_prob = np.max(seq_prob)\n",
    "            best_previous_state = np.argmax(seq_prob)\n",
    "\n",
    "            # Calculate the probability of the most likely sequence leading to this state at time t, including the current observation.\n",
    "            # Add eps to help with numerical issues.\n",
    "            V[t, state] = (max_seq_prob + eps) * (observation_probs[state, observed_seq[t]] + eps)\n",
    "\n",
    "            backpointer[t, state] = best_previous_state\n",
    "\n",
    "    t = num_obs - 1\n",
    "\n",
    "    # Initialise the sequence of predicted states\n",
    "    state_seq = np.zeros(num_obs, dtype=int)\n",
    "\n",
    "    # Get the most likely final state:\n",
    "    state_seq[t] = np.argmax(V[t, :])\n",
    "\n",
    "    # Backtrack until the first observation\n",
    "    for t in range(len(observed_seq)-1, 0, -1):\n",
    "        state_seq[t-1] = backpointer[t, state_seq[t]]\n",
    "\n",
    "    return state_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 2.6:** Use the viterbi function to estimate the most likely sequence of states on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 11468/11468 [00:31<00:00, 367.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR OWN CODE HERE\n",
    "predictions = []\n",
    "for sentence in tqdm(test_toks_encoded):\n",
    "    predictions.append(viterbi(sentence, num_tags, start_states, transitions, observations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will convert the predicted tag IDs to names and print the predictions along with ground truth for selected examples so we can see where it made errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 11468/11468 [00:00<00:00, 12297.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'My', 'God', ',', \"I'm\", 'shot', \"''\", '!', '!']\n",
      "['.', 'DET', 'NOUN', '.', 'PRT', 'VERB', '.', '.', '.']\n",
      "['.' 'NOUN' 'NOUN' '.' 'PRT' 'VERB' '.' '.' '.']\n",
      "['She', 'thought', 'she', 'was', 'bigger', 'than', 'we', 'are', 'because', 'she', 'came', 'from', 'Torino', \"''\", '.']\n",
      "['PRON', 'VERB', 'PRON', 'VERB', 'ADJ', 'ADP', 'PRON', 'VERB', 'ADP', 'PRON', 'VERB', 'ADP', 'NOUN', '.', '.']\n",
      "['PRON' 'VERB' 'PRON' 'VERB' 'ADJ' 'ADP' 'PRON' 'VERB' 'ADV' 'PRON' 'VERB'\n",
      " 'ADP' 'NOUN' '.' '.']\n",
      "['Meanwhile', ',', 'I', 'reloaded', 'my', 'gun', ',', 'as', 'the', 'other', 'men', 'were', 'doing', '.']\n",
      "['ADV', '.', 'PRON', 'VERB', 'DET', 'NOUN', '.', 'ADP', 'DET', 'ADJ', 'NOUN', 'VERB', 'VERB', '.']\n",
      "['ADV' '.' 'PRON' 'VERB' 'DET' 'NOUN' '.' 'ADV' 'DET' 'ADJ' 'NOUN' 'VERB'\n",
      " 'VERB' '.']\n",
      "['The', 'difficulty', 'was', 'that', 'each', 'day', 'seemed', 'to', 'produce', 'its', 'quota', 'of', 'details', 'which', 'must', 'be', 'cleaned', 'up', 'immediately', '.']\n",
      "['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN', 'VERB', 'PRT', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'DET', 'VERB', 'VERB', 'VERB', 'PRT', 'ADV', '.']\n",
      "['DET' 'NOUN' 'VERB' 'PRON' 'DET' 'NOUN' 'VERB' 'PRT' 'VERB' 'DET' 'NOUN'\n",
      " 'ADP' 'NOUN' 'DET' 'VERB' 'VERB' 'VERB' 'PRT' 'ADV' '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the sequence of tag IDs to tag names\n",
    "predicted_tags = []\n",
    "for sequence in tqdm(predictions):\n",
    "    predicted_tags.append(tag_encoder.inverse_transform(sequence))\n",
    "\n",
    "# print some examples:\n",
    "examples = [2, 334, 4983, 2389]\n",
    "for eg in examples:\n",
    "    print(test_toks[eg])\n",
    "    print(test_tags[eg])\n",
    "    print(predicted_tags[eg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well it did overall by computing performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9015292609995729\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "all_predictions = [tag for sentence in predictions for tag in sentence]\n",
    "all_targets = [tag for sentence in test_tags_encoded for tag in sentence]\n",
    "\n",
    "acc = accuracy_score(all_targets, all_predictions)\n",
    "print(f'Accuracy = {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. POS Tagging with an RNN\n",
    "The code below is adapted from last week's text classifier code to first pad the sequences, then format them into DataLoader objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sequence_length = 40  # truncate all docs longer than this. Pad all docs shorter than this.\n",
    "padding_token_id = V  # one value higher than the other tags\n",
    "\n",
    "def pad_sequence(sequence):\n",
    "    # pad with -1s\n",
    "    if len(sequence) >= sequence_length:\n",
    "        sequence = sequence[:sequence_length]\n",
    "    else:\n",
    "        sequence = np.concatenate((np.zeros(sequence_length-len(sequence)) + padding_token_id, sequence))\n",
    "    return sequence\n",
    "\n",
    "padded_train_toks_encoded = [pad_sequence(toks)[None, :] for toks in train_toks_encoded]\n",
    "padded_train_tags_encoded = [pad_sequence(tags)[None, :] for tags in train_tags_encoded]\n",
    "\n",
    "padded_test_toks_encoded = [pad_sequence(toks)[None, :] for toks in test_toks_encoded]\n",
    "padded_test_tags_encoded = [pad_sequence(tags)[None, :] for tags in test_tags_encoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# convert from the Huggingface format to a TensorDataset so we can use the mini-batch sampling functionality\n",
    "def convert_to_data_loader(inputs, labels):\n",
    "    inputs = np.concatenate(inputs, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "    # convert from array to tensor\n",
    "    input_tensor = torch.from_numpy(inputs).long()\n",
    "    label_tensor = torch.from_numpy(labels).long()\n",
    "    tensor_dataset = TensorDataset(input_tensor, label_tensor)\n",
    "    loader = DataLoader(tensor_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return loader\n",
    "\n",
    "train_loader = convert_to_data_loader(padded_train_toks_encoded, padded_train_tags_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_loader = convert_to_data_loader(padded_test_toks_encoded, padded_test_tags_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we're going to create a neural network sequence tagger using an RNN layer. This will be based on the code we used last time, with two key differences:\n",
    "  * Including an RNN hidden layer\n",
    "  * The output will have an additional dimension of size sequence_length, so that it can provide predictions for every token in the sequence.\n",
    "\n",
    "**TODO 3.1:** Complete the code below to change the hidden layer to a single RNN layer. See [the documentation](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class FFTextClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, num_classes):\n",
    "        super(FFTextClassifier, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Here we just need to construct the components of our network. We don't need to connect them together yet.\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_size) # embedding layer\n",
    "\n",
    "        ### COMPLETE YOUR CODE HERE\n",
    "        self.hidden_layer = nn.RNN(embedding_size, hidden_size, num_layers=1, batch_first=True) # Hidden layer\n",
    "        ###\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes) # Full connection layer\n",
    "\n",
    "    def forward (self, input_words):\n",
    "        # Input dimensions are:  (batch_size, seq_length)\n",
    "        embedded_words = self.embedding_layer(input_words)  # (batch_size, seq_length, embedding_size)\n",
    "\n",
    "        ### COMPLETE THE CODE HERE TO USE THE HIDDEN RNN LAYER\n",
    "        h, _ = self.hidden_layer(embedded_words)   # (batch_size, seq_length, hidden_size)\n",
    "        ###\n",
    "\n",
    "        output = self.output_layer(h)                      # (batch_size, seq_length, num_classes)\n",
    "        output = torch.transpose(output, 1, 2)              # (batch_size, num_classes, seq_length)\n",
    "        # Notice we haven't applied a softmax activation to the output layer -- it's not required by Pytorch's loss function.\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we can run the code below to train and test the RNN model. This uses basically the same code as last week.\n",
    "\n",
    "**TO-DO 3.2:** What is wrong with comparing the RNN tagger's performance computed here with that of the HMM? Hint: all the sequences are length 40.\n",
    "\n",
    "**TO-DO 3.3:** Can you fix the accuracy computations to make them comparable with the accuracy for the HMM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embedding_size = 25  # number of dimensions for embeddings\n",
    "hidden_size = 32 # number of hidden units\n",
    "\n",
    "ff_classifier_model = FFTextClassifier(V+1, embedding_size, hidden_size, num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_nn(num_epochs, model, train_dataloader, dev_dataloader, loss_fn, optimizer):\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "        # Track performance on the training set as we are learning...\n",
    "        total_correct = 0\n",
    "        total_trained = 0\n",
    "        train_losses = []\n",
    "\n",
    "        model.train()  # Put the model in training mode.\n",
    "\n",
    "        for i, (batch_input_ids, batch_labels) in enumerate(train_dataloader):\n",
    "            # Iterate over each batch of data\n",
    "            # print(f'batch no. = {i}')\n",
    "\n",
    "            optimizer.zero_grad()  # Reset the optimizer\n",
    "\n",
    "            # Use the model to perform forward inference on the input data.\n",
    "            # This will run the forward() function.\n",
    "            output = model(batch_input_ids)\n",
    "\n",
    "            # Compute the loss for the current batch of data\n",
    "            batch_loss = loss_fn(output, batch_labels)\n",
    "\n",
    "            # Perform back propagation to compute the gradients with respect to each weight\n",
    "            batch_loss.backward()\n",
    "\n",
    "            # Update the weights using the compute gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record the loss from this sample to keep track of progress.\n",
    "            train_losses.append(batch_loss.item())\n",
    "\n",
    "            # Count correct labels so we can compute accuracy on the training set\n",
    "            predicted_labels = output.argmax(1)\n",
    "                        \n",
    "            ### CHANGE CODE HERE\n",
    "            # n_tokens_in_batch = batch_labels.size(0) * sequence_length\n",
    "            n_tokens_in_batch = np.sum(batch_input_ids.numpy() != padding_token_id)\n",
    "            predicted_labels = predicted_labels[batch_input_ids != padding_token_id]\n",
    "            batch_labels = batch_labels[batch_input_ids != padding_token_id]\n",
    "            ###\n",
    "            \n",
    "            total_correct += (predicted_labels == batch_labels).sum().item()\n",
    "            total_trained += n_tokens_in_batch\n",
    "\n",
    "        train_accuracy = total_correct/total_trained*100\n",
    "\n",
    "        print(\"Epoch: {}/{}\".format((e+1), num_epochs),\n",
    "              \"Training Loss: {:.4f}\".format(np.mean(train_losses)),\n",
    "              \"Training Accuracy: {:.4f}%\".format(train_accuracy))\n",
    "\n",
    "        model.eval()  # Switch model to evaluation mode\n",
    "        total_correct = 0\n",
    "        total_trained = 0\n",
    "        dev_losses = []\n",
    "\n",
    "        for dev_input_ids, dev_labels in dev_dataloader:\n",
    "            dev_output = model(dev_input_ids)\n",
    "            dev_loss = loss_fn(dev_output, dev_labels)\n",
    "\n",
    "            # Save the loss on the dev set\n",
    "            dev_losses.append(dev_loss.item())\n",
    "\n",
    "            # Count the number of correct predictions\n",
    "            predicted_labels = dev_output.argmax(1)\n",
    "            \n",
    "            ### CHANGE CODE HERE\n",
    "            # n_tokens_in_batch = dev_labels.size(0) * sequence_length\n",
    "            n_tokens_in_batch = np.sum(dev_input_ids.numpy() != padding_token_id)\n",
    "            predicted_labels = predicted_labels[dev_input_ids != padding_token_id]\n",
    "            dev_labels = dev_labels[dev_input_ids != padding_token_id]           \n",
    "            ###\n",
    "            \n",
    "            total_correct += (predicted_labels == dev_labels).sum().item()\n",
    "            total_trained += n_tokens_in_batch\n",
    "\n",
    "        dev_accuracy = total_correct/total_trained*100\n",
    "\n",
    "        print(\"Epoch: {}/{}\".format((e+1), num_epochs),\n",
    "              \"Validation Loss: {:.4f}\".format(np.mean(dev_losses)),\n",
    "              \"Validation Accuracy: {:.4f}%\".format(dev_accuracy))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below runs the trainin process by calling train_nn():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 Training Loss: 1.3626 Training Accuracy: 57.0481%\n",
      "Epoch: 1/10 Validation Loss: 0.8516 Validation Accuracy: 71.8970%\n",
      "Epoch: 2/10 Training Loss: 0.6870 Training Accuracy: 77.3905%\n",
      "Epoch: 2/10 Validation Loss: 0.5705 Validation Accuracy: 81.1442%\n",
      "Epoch: 3/10 Training Loss: 0.4981 Training Accuracy: 83.4649%\n",
      "Epoch: 3/10 Validation Loss: 0.4466 Validation Accuracy: 85.1668%\n",
      "Epoch: 4/10 Training Loss: 0.3992 Training Accuracy: 86.7975%\n",
      "Epoch: 4/10 Validation Loss: 0.3738 Validation Accuracy: 87.6466%\n",
      "Epoch: 5/10 Training Loss: 0.3348 Training Accuracy: 88.9441%\n",
      "Epoch: 5/10 Validation Loss: 0.3242 Validation Accuracy: 89.2042%\n",
      "Epoch: 6/10 Training Loss: 0.2882 Training Accuracy: 90.4683%\n",
      "Epoch: 6/10 Validation Loss: 0.2880 Validation Accuracy: 90.4718%\n",
      "Epoch: 7/10 Training Loss: 0.2527 Training Accuracy: 91.6521%\n",
      "Epoch: 7/10 Validation Loss: 0.2609 Validation Accuracy: 91.3927%\n",
      "Epoch: 8/10 Training Loss: 0.2251 Training Accuracy: 92.5832%\n",
      "Epoch: 8/10 Validation Loss: 0.2401 Validation Accuracy: 92.1357%\n",
      "Epoch: 9/10 Training Loss: 0.2027 Training Accuracy: 93.3380%\n",
      "Epoch: 9/10 Validation Loss: 0.2241 Validation Accuracy: 92.7032%\n",
      "Epoch: 10/10 Training Loss: 0.1845 Training Accuracy: 93.9114%\n",
      "Epoch: 10/10 Validation Loss: 0.2117 Validation Accuracy: 93.1084%\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "learning_rate = 0.0005\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=V)\n",
    "optimizer = optim.Adam(ff_classifier_model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 10\n",
    "trained_model = train_nn(num_epochs, ff_classifier_model, train_loader, test_loader, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below implements a testing or prediction function and computes accuracy.\n",
    "\n",
    "**TO-DO 3.4:** Adjust the code below to correctly compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 93.11%\n"
     ]
    }
   ],
   "source": [
    "def test_nn(trained_model, test_loader, loss_fn):\n",
    "\n",
    "    trained_model.eval()\n",
    "\n",
    "    test_losses = []\n",
    "    correct = 0  # count the number of correct classification labels\n",
    "    total_labels = 0\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        test_output = trained_model(inputs)\n",
    "        loss = loss_fn(test_output, labels)\n",
    "        test_losses.append(loss.item())\n",
    "        predicted_labels = test_output.argmax(1)\n",
    "\n",
    "        ### CHANGE THE CODE BELOW\n",
    "        # n_tokens_in_batch = labels.size(0) * sequence_length\n",
    "        n_tokens_in_batch = np.sum(inputs.numpy() != padding_token_id)\n",
    "        predicted_labels = predicted_labels[inputs != padding_token_id]\n",
    "        labels = labels[inputs != padding_token_id]   \n",
    "        ###\n",
    "        \n",
    "        count_correct = torch.sum(predicted_labels == labels).item()\n",
    "        correct += count_correct\n",
    "        total_labels += n_tokens_in_batch\n",
    "\n",
    "    accuracy = correct/total_labels * 100\n",
    "    print(\"Test Accuracy: {:.2f}%\".format(accuracy))\n",
    "    # print(predicted)\n",
    "\n",
    "test_nn(trained_model, test_loader, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dialogue_and_narrative",
   "language": "python",
   "name": "dialogue_and_narrative"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
