{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "In this lab we will use both sparse vectors and dense word2vec embeddings to obtain vector representations of words and documents.\n",
    "\n",
    "### Outcomes\n",
    "* Be able to compute term-document matrices from a collection of text documents.\n",
    "* Be able to implement cosine similarity.\n",
    "* Know how to use Gensim to train, download and apply word embedding models.\n",
    "* Understand the word analogy task for word embeddings.\n",
    "\n",
    "### Overview\n",
    "\n",
    "First, we will load another set of tweet data. Then, we will obtain a term-document matrix, and compute cosine similarities. Then, we will use the Gensim library to train a word2vec model and download a pretrained model. Finally, we use the Gensim embeddings to perform the analogy task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data\n",
    "\n",
    "Instead of the sentiment classification dataset, we will work with the smaller emotion classification dataset. The dataset labels tweets as one of the following classes:\n",
    " * 0: anger\n",
    " * 1: joy\n",
    " * 2: optimism\n",
    " * 3: sadness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (./data_cache/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset with 3257 instances loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (./data_cache/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset with 1421 instances loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3257/3257 [00:00<00:00, 11158.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "train_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"emotion\",\n",
    "    split=\"train\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")\n",
    "\n",
    "test_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"emotion\",\n",
    "    split=\"test\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "print(f\"Test dataset with {len(test_dataset)} instances loaded\")\n",
    "\n",
    "# Put the data into lists ready for the next steps...\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "    train_texts.append(train_dataset[i]['text'])\n",
    "    train_labels.append(train_dataset[i]['label'])\n",
    "\n",
    "    # if i % 1000 == 0:\n",
    "    #     print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Term-Document Matrix\n",
    "\n",
    "**TO-DO 1.1:** Use the CountVectorizer, as in week 3, to obtain a term-document matrix for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2184)\t1\n",
      "  (0, 3337)\t1\n",
      "  (0, 3837)\t1\n",
      "  (0, 3964)\t1\n",
      "  (0, 4166)\t1\n",
      "  (0, 4514)\t1\n",
      "  (0, 4600)\t1\n",
      "  (0, 4761)\t1\n",
      "  (0, 4926)\t1\n",
      "  (0, 5203)\t1\n",
      "  (0, 5423)\t1\n",
      "  (0, 5758)\t1\n",
      "  (0, 8191)\t2\n",
      "  (0, 8284)\t1\n",
      "  (1, 622)\t1\n",
      "  (1, 784)\t1\n",
      "  (1, 1177)\t1\n",
      "  (1, 2722)\t1\n",
      "  (1, 3337)\t1\n",
      "  (1, 3856)\t1\n",
      "  (1, 4819)\t1\n",
      "  (1, 5187)\t1\n",
      "  (1, 6275)\t1\n",
      "  (1, 6873)\t1\n",
      "  (1, 7324)\t1\n",
      "  :\t:\n",
      "  (3255, 7357)\t1\n",
      "  (3255, 8101)\t1\n",
      "  (3255, 8272)\t1\n",
      "  (3255, 8284)\t2\n",
      "  (3256, 513)\t1\n",
      "  (3256, 843)\t1\n",
      "  (3256, 1132)\t1\n",
      "  (3256, 2160)\t1\n",
      "  (3256, 2680)\t1\n",
      "  (3256, 3236)\t1\n",
      "  (3256, 3766)\t1\n",
      "  (3256, 4060)\t1\n",
      "  (3256, 4247)\t1\n",
      "  (3256, 4269)\t1\n",
      "  (3256, 5160)\t1\n",
      "  (3256, 5570)\t1\n",
      "  (3256, 6502)\t1\n",
      "  (3256, 6800)\t1\n",
      "  (3256, 7362)\t1\n",
      "  (3256, 7365)\t2\n",
      "  (3256, 7396)\t1\n",
      "  (3256, 7413)\t1\n",
      "  (3256, 7815)\t2\n",
      "  (3256, 7993)\t1\n",
      "  (3256, 8116)\t1\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR ANSWER HERE\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorizer.fit(train_texts)\n",
    "X_train = vectorizer.transform(train_texts)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 1.2:** Print out the term vector for the word 'happy'. Use the vocabulary_ attribute to look up the word's index. \n",
    "*Hint:* the CountVectorizer stores a term-document matrix in a sparse format to save memory. You can convert this to a standard numpy array using the method '.toarray()'.\n",
    "*Hint:* you can use the method '.flatten()' to convert a 1xN matrix to a vector.\n",
    "\n",
    "The print-out probably won't be terribly readable, so you will need to convince yourself you have obtained the correct vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "(3257,)\n",
      "(3257, 8328)\n",
      "8328\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR ANSWER HERE\n",
    "index = vectorizer.vocabulary_['happy']\n",
    "vec = X_train[:, index].toarray().flatten()\n",
    "print(vec)\n",
    "print(vec.shape)\n",
    "print(X_train.shape)\n",
    "print(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO-DO 1.3:** Print out the document vector for the first tweet in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR ANSWER HERE\n",
    "index = 0\n",
    "X_train[0, :].toarray().flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cosine Similarity\n",
    "\n",
    "**TO-DO 2.1:** Write a function that computes cosine similarity between two vectors. *Hint:* you might find numpy's linalg library useful. Refer to the textbook for a definition of cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "### WRITE YOUR OWN CODE HERE\n",
    "\n",
    "def similarity(x, y):\n",
    "    dot_prod = np.dot(x, y)\n",
    "    return dot_prod / (np.linalg.norm(x)*np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 2.2:** Use the function to find the five most similar words to 'happy' according to the document-term matrix. *Hint:* the vocab_inverted dictionary that we compute below lets you look up a word given its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8328/8328 [00:02<00:00, 3637.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hopelessness\n",
      "anxietyprobz\n",
      "ampalaya\n",
      "paitpaitanangpeg\n",
      "birthday\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# invert the vocabulary dictionary so we can look up word types given an index\n",
    "keys = vectorizer.vocabulary_.values()\n",
    "values = vectorizer.vocabulary_.keys()\n",
    "vocab_inverted = dict(zip(keys, values))\n",
    "\n",
    "# WRITE YOUR OWN CODE HERE\n",
    "\n",
    "index = vectorizer.vocabulary_['happy']\n",
    "happy_vector = X_train[:, index].toarray().flatten()\n",
    "vocab_size = X_train.shape[1]\n",
    "\n",
    "similarities = np.zeros(vocab_size)\n",
    "for i in tqdm(range(vocab_size)):\n",
    "    if i == index:\n",
    "        continue  # skip comparison with the input word\n",
    "\n",
    "    similarities[i] = similarity(happy_vector, X_train[:, i].toarray().flatten())\n",
    "\n",
    "most_similar_idxs = np.argsort(similarities)[-5:]\n",
    "for idx in most_similar_idxs:\n",
    "    print(vocab_inverted[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Word2Vec\n",
    "\n",
    "For this part, we will need the gensim library. The code below tokenizes the training texts, then runs word2vec (the skipgram model) to learn a set of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "tokenized_texts = [list(tokenize(text, lowercase=True)) for text in train_texts]\n",
    "emb_model = word2vec.Word2Vec(tokenized_texts, seed=123, sg=1, min_count=1, window=3, vector_size=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look up the embedding for any given word like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.20619911,  0.01198039, -0.4783855 , -0.2384516 , -0.33177167,\n",
       "       -0.21107505, -1.1538926 , -0.4510568 ,  0.21269228,  0.02695287,\n",
       "       -0.16724521,  0.53105634,  0.38963693,  0.7182249 , -0.235123  ,\n",
       "       -0.01076802, -0.09377442, -0.23101725, -0.23545082,  0.19366637,\n",
       "       -0.39208633, -0.3270961 ,  0.15147023,  0.44964668, -0.5845829 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model.wv['happy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed above that we used gensim's own tokenizer. This means we have a slightly different vocabulary to the one produced by CountVectorizer. To access the vocab, we can use the following property: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user', 'i', 'the', 'to', 'a', 'and', 'you', 'is', 'of', 'it']\n"
     ]
    }
   ],
   "source": [
    "vocab = emb_model.wv.index_to_key\n",
    "print(vocab[:10])\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO-DO 3.1:** Now, use your cosine similarity method again to find the five most similar words to 'happy' according to your word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8174it [00:00, 88136.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worry, 0.9966073036193848\n",
      "keep, 0.9966935515403748\n",
      "yet, 0.9972906708717346\n",
      "mourn, 0.9973416924476624\n",
      "sorry, 0.9975515007972717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('sorry', 0.9975515007972717),\n",
       " ('mourn', 0.9973416924476624),\n",
       " ('yet', 0.9972906708717346),\n",
       " ('keep', 0.9966936111450195),\n",
       " ('worry', 0.9966073632240295)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR OWN CODE HERE\n",
    "\n",
    "happy_vector = emb_model.wv['happy']\n",
    "\n",
    "similarities = np.zeros(vocab_size)\n",
    "for i, wordtype in tqdm(enumerate(vocab)):\n",
    "    if wordtype == 'happy':\n",
    "        continue  # skip comparison with the input word\n",
    "\n",
    "    similarities[i] = similarity(happy_vector, emb_model.wv[wordtype])\n",
    "\n",
    "most_similar_values = np.sort(similarities)[-5:]\n",
    "most_similar_idxs = np.argsort(similarities)[-5:]\n",
    "for i, idx in enumerate(most_similar_idxs):\n",
    "    print(f'{vocab[idx]}, {most_similar_values[i]}')\n",
    "    \n",
    "# OR you can cheat and use the library function...\n",
    "emb_model.wv.most_similar('happy', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO-DO 3.2:** Have either of these embeddings been  effective at finding similar words? What might improve them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Downloading Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Above, we trained our own model using the skipgram method. We can also download a pretrained model that has previously been trained on a large corpus. There is a list of models available [here](https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models). Let's try out GLoVe embeddings (another way of learning embeddings than using the skipgram model) trained on a corpus of tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.2304   0.48312  0.14102 -0.0295  -0.65253 -0.18554  2.1033   1.7516\n",
      " -1.3001  -0.32113 -0.84774  0.41995 -3.8823   0.19638 -0.72865 -0.85273\n",
      "  0.23174 -1.0763  -0.83023  0.10815 -0.51015  0.27691 -1.1895   0.98094\n",
      " -0.13955]\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "glove_wv = gensim.downloader.load('glove-twitter-25')\n",
    "\n",
    "# show the vector for Hamlet:\n",
    "print(glove_wv['happy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 4.1:** Repeat the exercise above to find the closest relations to 'happy' with the downloaded model. How do the results compare to the embeddings we trained ourselves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('birthday', 0.9577818512916565),\n",
       " ('thank', 0.937666654586792),\n",
       " ('welcome', 0.93361496925354),\n",
       " ('love', 0.9176183342933655),\n",
       " ('miss', 0.9164500832557678)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "glove_wv.most_similar('happy', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5. Analogy Task\n",
    "\n",
    "An analogy can be formalised as:\n",
    "\n",
    "A is to B as A* is to B*.\n",
    "\n",
    "The analogy task is to find B* given A, B and A*.\n",
    "\n",
    "**TO-DO 5.1:** Define a function that can find the top N closest words B* for any given A, B and A*, using the Gensim embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1193514it [00:11, 102008.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['columnists', 'telesales', 'xstrology', 'caregiver', 'multilingual', 'optimization', 'marketer', 'programmer', 'traveller', 'sourcing']\n"
     ]
    }
   ],
   "source": [
    "vocab = glove_wv.index_to_key\n",
    "\n",
    "def analogy(A, B, Astar, embeddings, topn):\n",
    "    # WRITE YOUR OWN CODE HERE\n",
    "    target = embeddings[Astar] + embeddings[B] - embeddings[A]\n",
    "\n",
    "    similarities = np.zeros(len(vocab))\n",
    "    closest_word = '<NONE>'\n",
    "    for i, key in tqdm(enumerate(vocab)):\n",
    "        similarities[i] = similarity(target, embeddings[key])\n",
    "\n",
    "    idxs = np.argsort(similarities)[-topn:]\n",
    "    closest_words = [vocab[i] for i in idxs]\n",
    "        \n",
    "    return closest_words\n",
    "\n",
    "print(analogy('man', 'programmer', 'woman', glove_wv, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dialogue_and_narrative",
   "language": "python",
   "name": "dialogue_and_narrative"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
