{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Language Model\n",
    "\n",
    "In this lab we will implement a bigram language model and use it to compute the probability of some sample sentences.\n",
    "\n",
    "As you go through, make sure you understand what's going on in each cell, and ask if it is unclear.\n",
    "\n",
    "### Outcomes\n",
    "* Know how to count word frequencies in a corpus using Python libraries.\n",
    "* Understand how to compute conditional probabilities.\n",
    "* Be able to apply the chain rule to compute the probability of a sentence.\n",
    "\n",
    "### Overview\n",
    "\n",
    "The first part of the notebook loads the same dataset as last week.\n",
    "The next part splits the data into training and test sets, and tokenises the utterances.\n",
    "After this there are some tasks to complete to implement and test the language model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "split = \"train\"\n",
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"doc2dial\",\n",
    "    name=\"dialogue_domain\",  # this is the name of the dataset for the second subtask, dialog generation\n",
    "    split=split,\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all the utterances into a list. \n",
    "# For this task, we don't care about the order of the utterances in the conversation -- \n",
    "# we will just be using the utterances of examples of the language we want to model.\n",
    "\n",
    "utterances = []\n",
    "for sample in dataset:\n",
    "    turns = sample['turns']\n",
    "    for turn in turns:\n",
    "        if turn['role'] == 'user':\n",
    "            utterances.append(turn['utterance'])\n",
    "            \n",
    "###\n",
    "print(f'Number of utterances: {len(utterances)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise the samples. You can replace NLTK with another tokenizer if you prefer. \n",
    "import nltk\n",
    "\n",
    "for i in range(len(utterances)):\n",
    "    utterances[i] = nltk.word_tokenize(utterances[i])\n",
    "    \n",
    "print(utterances[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to put in some artificial start <s> and end <e> tokens. \n",
    "# These will be used to compute conditional probabilities of each word at the start of a sentence, \n",
    "# and conditional probabilities of ending the sentence after a particular word. \n",
    "# This lets us model which words are most likely to start or end a sentence. \n",
    "\n",
    "for i in range(len(utterances)):\n",
    "    utterances[i] = ['<s>'] + utterances[i] + ['<e>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test using scikit-learn.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_size = 0.8\n",
    "test_size = 0.2\n",
    "\n",
    "# Split the train data from the test data\n",
    "train_data, test_data = train_test_split(utterances, train_size=train_size, test_size=test_size)\n",
    "\n",
    "\n",
    "print(f'The training set has {len(train_data)} samples and the test set has {len(test_data)} samples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Counting Tokens\n",
    "\n",
    "The bigram language model needs to compute two sets of counts from the training data:\n",
    "1. The counts of how many times each bigram occurs.\n",
    "2. The counts of how many times each word type occurs as the first token in a bigram. \n",
    "\n",
    "Let's start by finding the vocabulary of unique token 'types': "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocab = np.unique(np.concatenate(train_data))\n",
    "V = len(vocab)\n",
    "\n",
    "print(vocab)\n",
    "print(f'There are {V} types in our vocabulary.')\n",
    "\n",
    "# Currently, vocab is a numpy array. It may be simpler to work with list:\n",
    "vocab = vocab.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create an object to store the bigram counts in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A matrix where row indexes will correspond to the first token in a bigram, \n",
    "# and column indexes to the second token. The indexes must map to the index\n",
    "# of the token in the vocabulary. The values in the matrix will be the counts.\n",
    "counts = np.ones((V, V))  # set to ones for add one smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an example of how to find the index of a given word:\n",
    "\n",
    "word = '<s>'  # example word\n",
    "\n",
    "def get_index_for_word(word, vocab):\n",
    "    if word in vocab:\n",
    "        index = vocab.index(word)\n",
    "        # np.argwhere(vocab == word)[0][0]  # if vocab is a numpy array, we can find the index of the word like this\n",
    "    else:\n",
    "        index = -1\n",
    "    return index\n",
    "\n",
    "index = get_index_for_word(word, vocab)\n",
    "\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2.1:** count the bigrams that occur in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sen in train_data:\n",
    "    ### WRITE YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2.2:** Apply numpy's sum() function to the 'counts' variable to compute the number of times each word type occurs as the first token in a bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "first_token_counts = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2.3:** Compute a matrix (numpy array) of conditional probabilities using the counts. Compute the log of this matrix as a variable 'log_cond_probs'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "\n",
    "###\n",
    "print(cond_probs)\n",
    "print(log_cond_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2.4:** Write a function that uses log_cond_probs to compute the probability of a given tokenised sentence, such as the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example tokenised sentence\n",
    "sen = ['<s>', 'If', 'you', 'give', 'me', 'the', 'help', ',', 'what', 'is', 'the', 'payment', 'system', '?', '<e>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_log_prob_sen(sen, vocab, log_cond_probs):\n",
    "    ### WRITE YOUR CODE HERE\n",
    "\n",
    "    ###\n",
    "\n",
    "    return log_prob_sen\n",
    "\n",
    "log_prob_sen = compute_log_prob_sen(sen, vocab, log_cond_probs)\n",
    "print(np.exp(log_prob_sen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2.5:** Compute the perplexity over the whole test set. You will need to make sure your code can handle unknown words -- make sure it does not end up misusing the index of -1 returned by get_index_for_word() for unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def perplexity(sen, vocab, log_cond_probs):\n",
    "    \"\"\"\n",
    "    Compute perplexity for one sentence\n",
    "    \"\"\"\n",
    "    ### WRITE YOUR CODE HERE\n",
    "\n",
    "    \n",
    "    ###\n",
    "    return ppl\n",
    "\n",
    "# Use perplexity() function to compute average perplexity across whole test dataset\n",
    "### WRITE YOUR CODE HERE\n",
    "\n",
    "\n",
    "###\n",
    "print(avg_ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTENSION 1:** Use the language model to generate new sentences by sampling. \n",
    "You can follow the example below to sample using scipy's multinomial class. Replace the distribution with the conditional distribution we computed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multinomial\n",
    "\n",
    "example_vocab = np.array(['a', 'b', 'c', 'd'])\n",
    "\n",
    "distribution = [0.3, 0.2, 0.1, 0.4]\n",
    "sample = multinomial.rvs(1, distribution)\n",
    "sample_bool = sample.astype(bool)  # convert the sample from integer to boolean\n",
    "generated_word = example_vocab[sample_bool][0]  # use the one-hot boolean vector to look up the word\n",
    "\n",
    "print(generated_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "current_tok = '<s>'\n",
    "tokens = [current_tok]\n",
    "\n",
    "while current_tok != '<e>' and len(tokens) < 1000:\n",
    "    ### WRITE YOUR CODE HERE\n",
    "\n",
    "    \n",
    "    ###\n",
    "    tokens.append(current_tok)\n",
    "\n",
    "print(tokens)\n",
    "print(len(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MORE EXTENSIONS: \n",
    "* Add some smoothing to the counts and see how it affects the results.\n",
    "* Use trigrams instead of bigrams. Does it improve perplexity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dialogue_and_narrative",
   "language": "python",
   "name": "dialogue_and_narrative"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
