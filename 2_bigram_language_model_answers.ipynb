{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Language Model\n",
    "\n",
    "In this lab we will implement a bigram language model and use it to compute the probability of some sample sentences.\n",
    "\n",
    "As you go through, make sure you understand what's going on in each cell, and ask if it is unclear.\n",
    "\n",
    "### Outcomes\n",
    "* Know how to count word frequencies in a corpus using Python libraries.\n",
    "* Understand how to compute conditional probabilities.\n",
    "* Be able to apply the chain rule to compute the probability of a sentence.\n",
    "\n",
    "### Overview\n",
    "\n",
    "The first part of the notebook loads the same dataset as last week.\n",
    "The next part splits the data into training and test sets, and tokenises the utterances.\n",
    "After this there are some tasks to complete to implement and test the language model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset doc2dial/dialogue_domain (download: 5.61 MiB, generated: 7.86 MiB, post-processed: Unknown size, total: 13.47 MiB) to ./data_cache/doc2dial/dialogue_domain/1.0.1/765cb4d9af421b599d910080fd61b4a43440c1232693876470ef3245daa5fa4c...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bb62e2f6b4460ca39d3abfba4ffaa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650091853c8a48629a1ccda8b3289de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3474 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Cannot find data file. \nOriginal error:\n[Errno 2] No such file or directory: 'data_cache/downloads/extracted/16042defdbe73da99c5a717927e4933cf58bac8a078b8269d50d7d0ef95f458e/doc2dial/v1.0.1/doc2dial_dial_train.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m cache_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdoc2dial\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdialogue_domain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# this is the name of the dataset for the second subtask, dialog generation\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_verifications\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dialogue_and_narrative/lib/python3.10/site-packages/datasets/load.py:1746\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[1;32m   1743\u001b[0m try_from_hf_gcs \u001b[38;5;241m=\u001b[39m path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 1746\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_verifications\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_verifications\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1755\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1756\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   1757\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dialogue_and_narrative/lib/python3.10/site-packages/datasets/builder.py:704\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF google storage unreachable. Downloading and preparing it from source\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m downloaded_from_gcs:\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_infos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_infos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dialogue_and_narrative/lib/python3.10/site-packages/datasets/builder.py:1227\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verify_infos)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verify_infos):\n\u001b[0;32m-> 1227\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_infos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_duplicate_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_infos\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dialogue_and_narrative/lib/python3.10/site-packages/datasets/builder.py:795\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verify_infos, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split(split_generator, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_split_kwargs)\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 795\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    796\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    798\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    799\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m    800\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# If check_duplicates is set to True , then except DuplicatedKeysError\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m DuplicatedKeysError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot find data file. \nOriginal error:\n[Errno 2] No such file or directory: 'data_cache/downloads/extracted/16042defdbe73da99c5a717927e4933cf58bac8a078b8269d50d7d0ef95f458e/doc2dial/v1.0.1/doc2dial_dial_train.json'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "split = \"train\"\n",
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"doc2dial\",\n",
    "    name=\"dialogue_domain\",  # this is the name of the dataset for the second subtask, dialog generation\n",
    "    split=split,\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all the utterances into a list. \n",
    "# For this task, we don't care about the order of the utterances in the conversation -- \n",
    "# we will just be using the utterances of examples of the language we want to model.\n",
    "\n",
    "utterances = []\n",
    "for sample in dataset:\n",
    "    turns = sample['turns']\n",
    "    for turn in turns:\n",
    "        if turn['role'] == 'user':\n",
    "            utterances.append(turn['utterance'])\n",
    "            \n",
    "###\n",
    "print(f'Number of utterances: {len(utterances)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise the samples. You can replace NLTK with another tokenizer if you prefer. \n",
    "import nltk\n",
    "\n",
    "for i in range(len(utterances)):\n",
    "    utterances[i] = nltk.word_tokenize(utterances[i])\n",
    "    \n",
    "print(utterances[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to put in some artificial start <s> and end <e> tokens. \n",
    "# These will be used to compute conditional probabilities of each word at the start of a sentence, \n",
    "# and conditional probabilities of ending the sentence after a particular word. \n",
    "# This lets us model which words are most likely to start or end a sentence. \n",
    "\n",
    "for i in range(len(utterances)):\n",
    "    utterances[i] = ['<s>'] + utterances[i] + ['<e>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test using scikit-learn.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_size = 0.8\n",
    "test_size = 0.2\n",
    "\n",
    "# Split the train data from the test data\n",
    "train_data, test_data = train_test_split(utterances, train_size=train_size, test_size=test_size)\n",
    "\n",
    "\n",
    "print(f'The training set has {len(train_data)} samples and the test set has {len(test_data)} samples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Counting Tokens\n",
    "\n",
    "The bigram language model needs to compute two sets of counts from the training data:\n",
    "1. The counts of how many times each bigram occurs.\n",
    "2. The counts of how many times each word type occurs as the first token in a bigram. \n",
    "\n",
    "Let's start by finding the vocabulary of unique token 'types': "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocab = np.unique(np.concatenate(train_data))\n",
    "V = len(vocab)\n",
    "\n",
    "print(vocab)\n",
    "print(f'There are {V} types in our vocabulary.')\n",
    "\n",
    "# Currently, vocab is a numpy array. It may be simpler to work with list:\n",
    "vocab = vocab.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create an object to store the bigram counts in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A matrix where row indexes will correspond to the first token in a bigram, \n",
    "# and column indexes to the second token. The indexes must map to the index\n",
    "# of the token in the vocabulary. The values in the matrix will be the counts.\n",
    "counts = np.ones((V, V))  # set to ones for add one smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an example of how to find the index of a given word:\n",
    "\n",
    "word = '<s>'  # example word\n",
    "\n",
    "def get_index_for_word(word, vocab):\n",
    "    if word in vocab:\n",
    "        index = vocab.index(word)\n",
    "        # np.argwhere(vocab == word)[0][0]  # if vocab is a numpy array, we can find the index of the word like this\n",
    "    else:\n",
    "        index = -1\n",
    "    return index\n",
    "\n",
    "index = get_index_for_word(word, vocab)\n",
    "\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2.1:** count the bigrams that occur in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sen in train_data:\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    for i in range(len(sen) - 1):\n",
    "        token_1 = sen[i]\n",
    "        token_2 = sen[i+1]\n",
    "        \n",
    "        index_1 = vocab.index(token_1)  # np.argwhere(vocab == token_1)[0][0]\n",
    "        index_2 = vocab.index(token_2)  # np.argwhere(vocab == token_2)[0][0]\n",
    "        \n",
    "        counts[index_1, index_2] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2.2:** Apply numpy's sum() function to the 'counts' variable to compute the number of times each word type occurs as the first token in a bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "first_token_counts = np.sum(counts, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2.3:** Compute a matrix (numpy array) of conditional probabilities using the counts. Compute the log of this matrix as a variable 'log_cond_probs'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00085121 0.00017024 0.00017024 ... 0.00017024 0.00017024 0.00017024]\n",
      " [0.00017209 0.00017209 0.00017209 ... 0.00017209 0.00017209 0.00017209]\n",
      " [0.00017173 0.00017173 0.00017173 ... 0.00017173 0.00017173 0.00017173]\n",
      " ...\n",
      " [0.00017209 0.00017209 0.00017209 ... 0.00017209 0.00017209 0.00017209]\n",
      " [0.00017212 0.00017212 0.00017212 ... 0.00017212 0.00017212 0.00017212]\n",
      " [0.00017212 0.00017212 0.00017212 ... 0.00017212 0.00017212 0.00017212]]\n",
      "[[-7.0688532  -8.67829111 -8.67829111 ... -8.67829111 -8.67829111\n",
      "  -8.67829111]\n",
      " [-8.66750795 -8.66750795 -8.66750795 ... -8.66750795 -8.66750795\n",
      "  -8.66750795]\n",
      " [-8.66957087 -8.66957087 -8.66957087 ... -8.66957087 -8.66957087\n",
      "  -8.66957087]\n",
      " ...\n",
      " [-8.66750795 -8.66750795 -8.66750795 ... -8.66750795 -8.66750795\n",
      "  -8.66750795]\n",
      " [-8.66733585 -8.66733585 -8.66733585 ... -8.66733585 -8.66733585\n",
      "  -8.66733585]\n",
      " [-8.66733585 -8.66733585 -8.66733585 ... -8.66733585 -8.66733585\n",
      "  -8.66733585]]\n"
     ]
    }
   ],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "cond_probs = counts / first_token_counts[:, None]\n",
    "log_cond_probs = np.log(cond_probs)\n",
    "print(cond_probs)\n",
    "print(log_cond_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2.4:** Write a function that uses log_cond_probs to compute the probability of a given tokenised sentence, such as the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example tokenised sentence\n",
    "sen = ['<s>', 'If', 'you', 'give', 'me', 'the', 'help', ',', 'what', 'is', 'the', 'payment', 'system', '?', '<e>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.619475216769052e-30\n"
     ]
    }
   ],
   "source": [
    "def compute_log_prob_sen(sen, vocab, log_cond_probs):\n",
    "    # WRITE YOUR CODE HERE\n",
    "    log_prob_sen = 0\n",
    "    for i in range(1, len(sen)):\n",
    "        index_0 = get_index_for_word(sen[i-1], vocab)\n",
    "        index_1 = get_index_for_word(sen[i], vocab)\n",
    "        \n",
    "        if index_0 == -1 or index_1 == -1:\n",
    "            continue  # handle unknown words\n",
    "            \n",
    "        log_prob_sen += log_cond_probs[index_0, index_1]\n",
    "\n",
    "    return log_prob_sen\n",
    "\n",
    "log_prob_sen = compute_log_prob_sen(sen, vocab, log_cond_probs)\n",
    "print(np.exp(log_prob_sen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2.5:** Compute the perplexity over the whole test set. You will need to make sure your code can handle unknown words -- make sure it does not end up misusing the index of -1 returned by get_index_for_word() for unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147.68326880114358\n"
     ]
    }
   ],
   "source": [
    "def perplexity(sen, vocab, log_cond_probs):\n",
    "    \"\"\"\n",
    "    Compute perplexity for one sentence\n",
    "    \"\"\"\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    log_prob = compute_log_prob_sen(sen, vocab, log_cond_probs)\n",
    "    ppl = np.exp(-log_prob * 1.0 / len(sen))  # perplexity of the sentence\n",
    "    ###\n",
    "    return ppl\n",
    "\n",
    "# Use perplexity() function to compute average perplexity across whole test dataset\n",
    "### WRITE YOUR CODE HERE\n",
    "total = 0\n",
    "for test_sen in test_data:\n",
    "    ppl = perplexity(test_sen, vocab, log_cond_probs)\n",
    "    total += ppl\n",
    "\n",
    "avg_ppl = total / len(test_data)  # mean across all test sentences\n",
    "###\n",
    "print(avg_ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTENSION 1:** Use the language model to generate new sentences by sampling. \n",
    "You can follow the example below to sample using scipy's multinomial class. Replace the distribution with the conditional distribution we computed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import multinomial\n",
    "\n",
    "example_vocab = np.array(['a', 'b', 'c', 'd'])\n",
    "\n",
    "distribution = [0.3, 0.2, 0.1, 0.4]\n",
    "sample = multinomial.rvs(1, distribution)\n",
    "sample_bool = sample.astype(bool)  # convert the sample from integer to boolean\n",
    "generated_word = example_vocab[sample_bool][0]  # use the one-hot boolean vector to look up the word\n",
    "\n",
    "print(generated_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'noteworthy', 'discount', 'Do', 'offenses', 'facilities', 'theme', 'reitrement', 'thats', 'gave', 'takes', '21', 'courts', 'stop', 'subvención', 'Washington', 'miss', 'g', 'full', 'internship', 'we', 'Intrastatal', 'class', 'professionals', 'mmmmh', 'graduate/professional', 'regarding', 'verified', 'labor', 'open', 'GE', 'special', 'Rules', 'Recharge', 'earn', 'deaf', 'liens', 'let', 'borrowers', 'registrant', 'doesnt', 'unexpired', 'D/E', 'troublesome', 'so', 'costly', 'recertifying', 'We', 'Hope', 'June', 'Assessments', 'exactly', 'our', 'exact', 'limousine', 'volunteer', 'labor', 'doubt', 'handicap', \"'ve\", 'sue', 'care', 'bullet', 'decision', 'Apply', '623', 'family', 'equipment', 'Eligibility', 'France', 'married', 'source', 'honors', 'un', 'Exit', 'services', 'react', 'release', 'areas', 'non-citizen', 'earned', 'texting', 'minor', 'taeaching', '20', 'Register', 'PIRP', 'Look', 'specially', 'started', '866', 'value', 'policy', 'false', 'following', 'temporary', 'February', 'co-signing', 'Down', 'lejeune', 'Que', 'CSR', 'burying', 'covering', 'suggested', 'instructions', 'unfortunately', 'situation', 'reqyuirements', 'registry', 'General', 'biological', 'Chicago', 'ok', 'most', 'transmission', 'preparing', 'in-transit', 'york', '1965', 'mySocial', 'Lifetime', 'O.k', 'divorce', 'challenge', 'repaying', 'past', 'TBV', 'fit', 'revoked', 'At', 'program', 'Plate', 'animals', 'contractors', 'handicapped', 'Unsubsidized', 'differently', 'procedure', 'amount', 'International', 'commiting', 'particular', 'Social', 'stalled', 'guardian', 'forward', 'stay', 'share', 'natural', 'assisting', 'odd', 'bonds', 'explore', 'Camp', 'starter', 'qualification', 'Nice', 'unexpired', 'party', 'Party', 'Adventure', 'overview', 'son', 'maybe', 'processed', 'party', 'funding', 'ways', 'front', 'Management', 'respond', 'smokemeter', 'testimonies', 'porch', 'Washington', 'coordinator', 'languages', 'name', 'taught', 'entrant', 'necessary', 'lets', 'readable', 'FAmily', 'environmental', 'applying', 'accessed', '20219-20', 'Unemployability', 'lump', '52', 'guaranty', 'nowhere', 'self-supporting', 'alright', 'Certification', 'organize', 'Interesting', 'UPDATE', 'strong', 'Direct', 'TTY', 'amending', 'cousin', 'Listen', 'Mustang', 'COVID-19', 'asks', 'reconstructing', 'recommendation', 'found', 'Allowance', '2009', 'Cuban-Haitian', 'discounts', 'evaluated', 'inquire', 'contains', 'something', 'owl-like', 'shold', 'ranges', 'Sales', 'Instructor', 'covered', 'REAL', 'peace', 'stole', 'bank', 'drawer', 'vision-related', 'exact', 'revision', 'shold', 'reference', 'Sales', 'Class', 'wit', 'enquiring', '30', 'Entrance', 'Operation', 'Examiner', 'inspection', 'este', 'Bureau', 'Informational', 'VET', '1151', 'closes', 'iv', 'back', 'Or', 'king', 'community', 'posible', 'registartion', 'plans', \"I'm\", 'former', 'killed', 'toes', 'carrier', 'Benefits', ']', 'guilty', 'Visitor', 'NOW', 'property', 'greater', 'belong', 'ex-wife', 'dsafe', 'gotten', 'low', 'Registry', 'supervises', 'tax', 'parties', 'publications', 'reuse', 'Meanwhile', 'liability', 'Plus', 'abbreviate', 'Even', 'payable', 'bummer', 'qualifying', 'interfere', 'details', 'tore', 'credited', 'supplied', 'Inspection', 'PIRPS', 'posible', 'enlighten', 'behalf', 'Evaluatoin', 'non-medical', 'exceeds', 'parties', 'seats', 'NOW', 'cosiste', 'downloadable', 'Computerized', 'honking', 'reactions', 'people', 'functional', 'brochure', 'implications', 'plus', 'troublesome', 'Temporary', 'Exhibition', 'Rehab', 'Speak', 'Who', 'social', 'knows', 'organizations', 'york', 'inspected', 'goods', 'HazMat', 'unable', 'communicate', 'Counseling', 'slightly', 'vmli', 'ready', 'ownership', 'shorter', 'liability', 'goes', 'Traumatic', 'boy', 'deciding', 'related', 'gay', 'This', 'Form', 'Road', 'organize', 'Need', 'sucks', 'Neither', 'secturity', 're-certification', 'valid', 'a', 'CDL', 'town', 'glad', 'fake', 'long', 'impose', 'financially', 'and/or', 'highlight', 'responsibilities', 'Jobs', 'contains', 'goods', 'Part', 'gaulify', 'versiion', 'ID', 'UPDATE', 'hurt', 'definitely', 'improvement', 'old', 'keeping', 'Employee', 'completing', 'according', 'incarceration', 'Indian', 'figured', 'graduate/professional', 'Better', '32', 'Suffolk', 'ad', 'recommendable', 'budget', 'interviewed', 'Member', 'depend', 'usual', 'excited', 'resolving', 'offering', 'communicate', 'projects', 'arose', 'territoriess', 'ow', 'taxis', 'manuals', 'researching', 'Interesting', 'caused', 'sobrevivientes', 'validate', 'bicycle', 'getting', 'oh', 'misdemeanor', 'vouchers', 'fees', 'yeah', 'COVID-19', 'books', 'VETERAN', 'proceeds', 'busy', '1955', 'type', 'husband', 'Serve', 'Aidan', 'teachers', 'out-of', 'cemetery', 'mainly', 'military', 'civilian', 'decided', 'away', 'Virtual', 'stalled', 'report', 'least', 'session', 're-entitled', 'Distributor', 'los', 'unused', 'separated', 'applicants', 'handled', 'reproductive', 't', 'graduates', 'standardized', 'NYC', 'Spanish', 'roommates', 'answers', 'Specially', 'dressing', 'works', 'men', 'be', 'separating', 'ones', 'tip', 'computers', 'NY', 'VA', 'want', 'Opacity', 'wife', 'improve', 'vessels', 'mail-order', 'DoD', 'ffel', 'enquiring', 'agencies', 'allows', 'wioth', 'travelling', 'misconduct', 'weeks', 'together', 'Document', 'planing', 'sample', 'effective', 'chronical', 'Hummm', 'recertifying', 'fly', 'starter', 'preventive', 'pertains', 'Fiduciary', 'Virginia', 'Data', 'ofr', 'excepted', 'score', 'bond', 'think', 'expecting', 'stickers', 'Greg', 'días', 'dad', 'Ford', 'department', 'offers', 'don', 'details', 'takes', 'whose', 'Decision', 'love', 'VEAP', 'ahold', 'lots', 'interacting', '2005', 'inividual', 'IHL', 'messages', 'address', 'Rehabilitation', 'Did', 'Ownership', 'base', 'overwhelmed', 'following', 'restaurants', 'relation', 'visiting', 'Downgrade', 'while', 'Employee', 'asdfa', 'request', 'were', 'DIAL', 'Enterprise', 'disaster', 'retire', 'Yeah', 'lots', 'O.K', 'monthly', 'counselor', 'session', 'Excuse', 'going', 'several', 'anyway', 'web', 'fasted', 'taxis', 'licensed', 'withdrawn', 'Lastly', 'caregiver', 'watched', 'ending', 'five', 'spine', 'ATS', 'reach', 'Born', 'holder', 'wishes', 'intern', 'Evaluatoin', 'cap', 'have', 'answered', 'affecting', 'graduated', 'shown', 'necesarios', 'post', 'lbs', 'denial', 'repayment', 'shops', 'debt', 'registry', 'decreased', 'DEMA', 'TRICARE', 'contact', 'lines', 'nydmv.messagingchannel.com', 'Another', 'responding', 'notify', 'incapacitated', 'discount', 'sexual', 'remember', 'also', 'doesn', 'long', 'don', 'combinations', 'Registration', 'Leandras', 'Work-Study', 'someday', 'though', 'RETURNED', 'activity', 'Anyway', 'yo', 'preferences', 'respect', 'entry', 'accessing', 'income-based', 'action', 'soon', 'director', 'mean', 'non-driver', 'Escort', 'sweated', 'O.K.', 'sendthe', 'Someone', 'reduced', 'died', 'retiremnt', 'impactful', 'lots', 'FSAA', 'anyway', 'Emissions', 'rumors', 'Lifetime', 'holder', 'specialty', 'motocycle', 'stepson', 'seniors', 'licenses', 'theirs', 'interesting', 'mission', 'defaulted', 'securing', 'adding', 'got', 'Listen', 'deciding', 'escort', 'France', 'dsafe', 'owe', 'somewhat', 'figured', 'still', 'office', 'Restrictions', 'Going', 'Letter', 'periods', 'year', 'identification', 'titles', 'accepted', 'culd', 'demilitarized', 'consideration', 'IDs', 'major', 'consequences', 'classified', 'struggling', 'represent', 'November', 'day', \"'\", 'mandated', 'Never', 'veteran', 'indicated', 'NY.gov', 'Nice', 'synopses', 'Any', 'Informational', 'Modifications', 'feature', \"n't\", '.', 'Because', 'Seller', 'hoping', 'wheels', 'worries', 'landlord', 'scholarship', 'dealer', 'Adopted', 'Suffered', 'servicing', 'Before', 'hearings', 'SSI', 'absolutely', 'Rational', 'answerable', 'Print', 'why', 'behind', 'Whee', 'ever', 'continued', 'sound', 'chose', 'Islands', 'Wait', 'issued', 'tab', 'difficulty', 'land', 'free', 'Debt-to-Earnings', '2019-2020', 'recorded', 'higher', 'permission', 'Claim', 'emmissions', 'benefits', '?', 'im', 'grandson', 'lack', 'orders', 'sorry', 'then', 'owned', 'any', 'selective', 'sought', 'answerable', 'Good', 'residents', 'othopedic', 'Premiums', 'attended', 'TTY', 'trouble', 'apprentice', 'SUV', 'test', 'Coaching', 'may', 'must', 'terminally', 'Track', 'acquisitions', 'pension', 'suspect', 'suffers', 'sustained', 'leandras', 'would', 'Jersey', 'diagnosed', 'proceeds', 'trauma', '772', 'Class', 'graduating', 'internship', 'dont', 'DD214', 'mostly', 'pounds', 'citizenshiip', 'kits', 'Perkins', 'Archives', 'higher', 'VA.', 'correcting', 'noncitizen', 'Income', 'TSGLI', 'isnt', 'Peace', 'extensions', 'Could', 'equipped', 'cant', 'isnt', 'supervised', 'US', 'fertility', 'anybody', 'courses', 'Acquisitions', 'present', 'opportunities', 'unwilling', 'Chinese', 'focus', 'folder', 'January', 'planned', 'boss', 'ATVs', 'expedited', 'non-excepted', 'turning', 'span', 'creates', 'error', 'birthday', 'men', 'Parks', 'turns', 'amend', 'Never', 'much', 'Education', 'We', 'life', 'badge', 'click', 'gathering', 'IDES', 'Micronesia', 'shift', 'Schools', 'purchases', 'experiences', 'Informed', 'perion', 'active-duty', 'dnt', 'ex-wife', 'reitrement', '28', 'recertify', 'sdgfgf', 'bit', 'proceeds', 'nydmv.messagingchannel.com', 'first', 'attorny', 'MANUFACTURER', 'immigration', 'waiver', 'stations', 'prudential', 'trouble', 'uncle', 'verifying', 'Directory', 'non-ED', '1,220', 'eligible', 'teacher', 'operator', 'serious', 'previously', 'teaching', 'Work', 'crosses', 'submission', 'technical', 'Premium', 'Driving', 'rights', 'motor', 'permission', 'leg', 'literally', 'instructions', 'Tricare', 'eleventh', 'username', 'dependency', 'trip', 'motorcyclel', 'ready', 'assigned', 'MYDMV', 'emission', 'rates', 'payed', 'This', 'Were', 'sporting', 'schoolarships', 'prices', 'figure', 'Lifecycle', 'madatory', 'old', 'advice', '2874', 'drived', 'wow', 'taught', 'Benefits', 'FY', 'Access', 'reininstate', 'leaves', 'Transfer', 'workshop', 'normal', 'significant', 'kin', 'directed', 'accepting', 'Back', 'believe', 'enough', 'standard', 'ratings', 'acquisition', 'without', 'purposes', 'recommendations', 'cancelled', 'part-time', 'photos', 'provisional', 'purpose', 'approval', 'Notification', 'wishes', 'MYDMV', 'Advocate', 'passing', 'preferred', 'imagine', 'appoint', 'immigration', 'subscription', 'stars', 'AA-33A', 'ownship', 'texting', 'error', 're-married', 'recommended', 'H', 'quesiton', 'qualifying', 'married', 'standardized', 'quality', \"'ve\", 'Reserves', 'incorrectly', '8', 'REAP', 'Violations', 'pertinent', 'knee.I', 'heck', 'aspire', 'Selective', 'purged', 'phrase', 'directly', 'activity', 'residents', 'Plan', 'throug', 'wage', 'Delayed', 'amended', 'gotten', 'die', 'operator', 'view', 'afraid', 'belonging', 'dishonourable', 'homebound', 'quality', 're-entitled', 'reviewed', 'getting', 'burying', 'ed', 'photographic', 'United', 'system', 'but', 'fire', 'FOR', 'Hm', 'Year', 'responsible']\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "current_tok = '<s>'\n",
    "tokens = [current_tok]\n",
    "\n",
    "while current_tok != '<e>' and len(tokens) < 1000:\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    distribution = np.exp(log_cond_probs[get_index_for_word(current_tok, vocab), :])\n",
    "    distribution = distribution / np.sum(distribution)  # normalise because numerical errors mean the values don't  quite sum to one\n",
    "\n",
    "    sample = multinomial.rvs(1, distribution)  # sample one word conditioned on current_tok\n",
    "    sample = np.where(sample)[0][0]\n",
    "    current_tok = vocab[sample]\n",
    "    \n",
    "    ###\n",
    "    tokens.append(current_tok)\n",
    "\n",
    "print(tokens)\n",
    "print(len(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MORE EXTENSIONS: \n",
    "* Add some smoothing to the counts and see how it affects the results.\n",
    "* Use trigrams instead of bigrams. Does it improve perplexity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dialogue_and_narrative",
   "language": "python",
   "name": "dialogue_and_narrative"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
